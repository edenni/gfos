# @package _global_

defaults:
  - override /model: layout
  - override /optimizer: adamw
  - override /scheduler: plateau
  - override /loss: batch_mer
  - override /logger: wandb
  - override /dataset: layout
  - override /trainer: layout

dataset:
  source: nlp
  search: default
  max_configs: 10240
  num_configs: 200
  three_split_sampling: true
  norm_method: mean_std

model:
  node_layer: SAGEConv
  num_node_layers: 4
  node_dim: 64

  config_neighbor_layer: GATConv
  num_config_neighbor_layers: 2
  config_neighbor_dim: 64

  config_layer: SAGEConv
  num_config_layers: 4
  config_dim: 64

  head_dim: 64
  dropout: 0.2
  activation: LeakyReLU

hydra:
  job:
    name: ${run_name} #_ft_fold_${dataset.fold}

logger:
  project: ${project}
  group: nlp_default_finetune
  name: ${run_name}
  tags:
    - pipeline
    - ${dataset.source}
    - ${dataset.search}
    - finetune

optimizer:
  lr: 1e-3
  weight_decay: 1e-7

scheduler:
  patience: 2

trainer:
  num_epochs: 800
  num_val_epochs: 40
  infer_bs: 100
  early_stopping: 4
  grad_clip: 0.0
  batch_size: 4
  load_path: G:\projects\gfos\logs\pipeline\nlp_ensemble\multiruns\2023-10-27_10-58-40\0\wandb\run-20231027_110222-0ytzj3l1\files\1679_0.9513.pth

tasks: ["train", "test"]  # available tasks(functions) defined in the pipeline, could be cv, train, test, tune etc.