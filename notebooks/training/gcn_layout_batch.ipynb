{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dir = r\"H:\\data\\gfos\\predict-ai-model-runtime\\npz_all\\npz\\layout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layout(\n",
    "    base_dir: str, compile_type: str, model_type: str | None = None\n",
    "):\n",
    "    if model_type is not None:\n",
    "        assert model_type in (\n",
    "            \"nlp\",\n",
    "            \"xla\",\n",
    "        ), f\"model_type must be nlp or xla but got {model_type}\"\n",
    "    assert compile_type in (\n",
    "        \"default\",\n",
    "        \"random\",\n",
    "    ), f\"compile_type must be default or random but got {compile_type}\"\n",
    "\n",
    "    dfs = defaultdict(list)\n",
    "\n",
    "    if model_type is None:\n",
    "        model_types = (\"nlp\", \"xla\")\n",
    "    else:\n",
    "        model_types = (model_type,)\n",
    "\n",
    "    dirs = [\n",
    "        os.path.join(base_dir, model_type, compile_type, training)\n",
    "        for model_type in model_types\n",
    "        for training in [\"train\", \"valid\", \"test\"]\n",
    "    ]\n",
    "\n",
    "    for path in dirs:\n",
    "        split = path.split(\"\\\\\")[-1]\n",
    "        files = os.listdir(path)\n",
    "\n",
    "        dfs[split] += [os.path.join(path, file) for file in files]\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "layout_xla_random = load_layout(\n",
    "    layout_dir,\n",
    "    compile_type=\"random\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutDataset(Dataset):\n",
    "    def __init__(self, files: list[str]):\n",
    "        self.files = files  # 💼 Initialize the dataset with a DataFrame containing the data\n",
    "        self.npzs = [np.load(file) for file in self.files]\n",
    "        self.num_records = [len(npz[\"config_runtime\"]) for npz in self.npzs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.npzs)  # 🔢 Define the length of the dataset, which is the number of rows in the DataFrame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # cum_records = np.cumsum(self.num_records)\n",
    "        # npz_idx = np.searchsorted(cum_records, idx)\n",
    "        # row = self.npzs[npz_idx]\n",
    "        \n",
    "        # cfg_idx = idx - cum_records[npz_idx - 1] if npz_idx > 0 else idx\n",
    "        row = self.npzs[idx]\n",
    "        config_feat = torch.tensor(row['node_config_feat'].astype(np.float32))  # 🧮 Convert and store 'config_feat' as a PyTorch tensor\n",
    "        node_config_ids = torch.tensor(row[\"node_config_ids\"].astype(np.int32))  # 🧮 Convert and store 'node_cfg_idx' as a PyTorch tensor\n",
    "        node_feat = torch.tensor(row['node_feat'].astype(np.float32))  # 🧮 Convert and store 'node_feat' as a PyTorch tensor\n",
    "        node_opcode = torch.tensor(row['node_opcode'].astype(np.int32))  # 🧮 Convert and store 'node_opcode' as a PyTorch tensor\n",
    "        edge_index = torch.tensor(np.swapaxes(row['edge_index'],0,1).astype(np.int32))  # 🧮 Convert and store 'edge_index' as a PyTorch tensor with axis swapping\n",
    "        target = row['config_runtime'].astype(np.float32)  # 📈 Calculate and store the target value with preprocessing\n",
    "        # 📊 Min-max scale the target value to ensure it's within a specific range (standardization)\n",
    "        target = (target - np.mean(target)) / (np.std(target) + 1e-5)\n",
    "        target = torch.tensor(target)  # 🧮 Convert and store the target as a PyTorch tensor\n",
    "        return config_feat, node_feat, node_opcode, edge_index, node_config_ids, target  # 🔁 Return the data and target for a specific sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, graph_feats, hidden_dim):\n",
    "        super().__init__()  # 🧬 Initialize the parent class 'torch.nn.Module'\n",
    "\n",
    "        op_embedding_dim = 4  # I choose 4-dimensional embedding\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            120,  # 120 different op-codes\n",
    "            op_embedding_dim,\n",
    "        )\n",
    "        assert len(hidden_channels) > 0\n",
    "        in_channels = op_embedding_dim + 140\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        last_dim = hidden_channels[0]\n",
    "\n",
    "        # Create a sequence of Graph Convolutional Network (GCN) layers\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels[0]))\n",
    "        for i in range(len(hidden_channels) - 1):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels[i], hidden_channels[i + 1])\n",
    "            )\n",
    "            last_dim = hidden_channels[i + 1]\n",
    "        self.convs.append(GCNConv(last_dim, graph_feats))\n",
    "\n",
    "        # Define a sequential dense neural network\n",
    "        self.dense = torch.nn.Sequential(\n",
    "            nn.Linear(graph_feats + 18, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(1, 1, 3, padding=1)\n",
    "\n",
    "    def forward(\n",
    "        self, x_cfg: torch.Tensor, x_feat: torch.Tensor, x_op: torch.Tensor, edge_index: torch.Tensor, node_config_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Get graph features\n",
    "        x = torch.cat(\n",
    "            [x_feat, self.embedding(x_op)], dim=1\n",
    "        )  # 📊 Concatenate input features with opcode embeddings\n",
    "\n",
    "        # Pass data through convolutional layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "\n",
    "        # Shape (n, nc, 64)\n",
    "        # x_graph = torch.index_select(x, 1, node_config_ids)\n",
    "        # x_cfg = x_cfg.flatten(1)\n",
    "        x_graph = torch.mean(x, dim=0)\n",
    "        x_cfg = torch.mean(x_cfg, dim=1).squeeze(1)\n",
    "\n",
    "        # Combine graph data with config data\n",
    "        x = torch.cat(\n",
    "            [x_cfg, x_graph.repeat((len(x_cfg), 1))], axis=1\n",
    "        )  # 🔄 Concatenate config data with repeated graph embeddings\n",
    "\n",
    "        # Pass the combined data through the dense neural network\n",
    "        x = torch.flatten(self.dense(x))\n",
    "\n",
    "        # Standardize the output\n",
    "        x = (x - torch.mean(x)) / (torch.std(x) + 1e-5)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the 'SimpleModel' and move it to the specified device (CPU or GPU)\n",
    "model = SimpleModel(\n",
    "    hidden_channels=[16, 32, 16, 48], graph_feats=64, hidden_dim=64\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Cross-Validation Training Loop (Enhanced)\n",
    "\n",
    "\n",
    "# Define the score_tile_mean function\n",
    "def score_tile_mean(predictions, df):\n",
    "    score = 0\n",
    "    for i in range(len(df)):\n",
    "        predbest = np.mean(df.iloc[i][\"config_runtime\"][predictions[i]])\n",
    "        best = np.mean(np.sort(df.iloc[i][\"config_runtime\"])[:5])\n",
    "        score += 2 - predbest / best\n",
    "    score /= len(df)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Define the score_tile_max function\n",
    "def score_tile_max(predictions, df):\n",
    "    score = 0\n",
    "    for i in range(len(df)):\n",
    "        predbest = np.min(df.iloc[i][\"config_runtime\"][predictions[i]])\n",
    "        best = np.min(df.iloc[i][\"config_runtime\"])\n",
    "        score += 2 - predbest / best\n",
    "    score /= len(df)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Create a K-Fold cross-validator with 5 splits\n",
    "# kfold = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Lists to store mean and max scores for each fold\n",
    "score_means = []\n",
    "score_maxs = []\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 5e-4  # Adjust the learning rate to a different value\n",
    "weight_decay = 1e-6  # Adjust weight decay to a different value\n",
    "num_epochs = 90  # You can keep the number of epochs as 90 or adjust as needed\n",
    "\n",
    "\n",
    "# Iterate through each fold\n",
    "# for fold, (tr_idx, va_idx) in enumerate(kfold.split(df)):\n",
    "train_dataset = LayoutDataset(layout_xla_random[\"train\"])\n",
    "val_dataset = LayoutDataset(layout_xla_random[\"valid\"])\n",
    "criterion = torch.nn.MSELoss()\n",
    "steps = len(train_dataset) * num_epochs  # Update the number of training steps\n",
    "warmup_steps = int(steps * 0.1)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "# scheduler = CosineLRScheduler(optimizer, t_initial=steps, warmup_t=warmup_steps, warmup_lr_init=1e-6, lr_min=2e-8)\n",
    "\n",
    "best_score = 0\n",
    "best_score_max = 0\n",
    "\n",
    "# Training loop with increased epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(range(len(train_dataset)), leave=False)\n",
    "    loss_sum = 0\n",
    "    n = 0\n",
    "\n",
    "    for i in pbar:\n",
    "        cfg_ft, nd_ft, nd_op, ind, cids, target = train_dataset[i]\n",
    "        cfg_ft, nd_ft, nd_op, ind, cids, target = (\n",
    "            cfg_ft.to(device),\n",
    "            nd_ft.to(device),\n",
    "            nd_op.to(device),\n",
    "            ind.to(device),\n",
    "            cids.to(device),\n",
    "            target.to(device),\n",
    "        )\n",
    "\n",
    "        out = model(cfg_ft, nd_ft, nd_op, ind, cids)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-2)\n",
    "        # scheduler.step(i + len(train_dataset) * epoch)\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        n += 1\n",
    "        pbar.set_description(\n",
    "            f\"running loss: {(loss_sum/n):.2f}, current loss: {(loss.item()):.2f}\"\n",
    "        )\n",
    "    pbar.close()\n",
    "    model.eval()\n",
    "    tile_xla_predictions = []\n",
    "    pbar = tqdm(range(len(val_dataset)), leave=False)\n",
    "\n",
    "    for i in pbar:\n",
    "        cfg_ft, nd_ft, nd_op, ind, cids, target = val_dataset[i]\n",
    "        cfg_ft, nd_ft, nd_op, ind, cids, target = (\n",
    "            cfg_ft.to(device),\n",
    "            nd_ft.to(device),\n",
    "            nd_op.to(device),\n",
    "            ind.to(device),\n",
    "            cids.to(device),\n",
    "            target.to(device),\n",
    "        )\n",
    "\n",
    "        out = model(cfg_ft, nd_ft, nd_op, ind, cids)\n",
    "        tile_xla_predictions.append(np.argsort(out.cpu().detach().numpy())[:5])\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Calculate and display scores for the current fold and epoch\n",
    "    score_mean = score_tile_mean(tile_xla_predictions, val_dataset.df)\n",
    "    score_max = score_tile_max(tile_xla_predictions, val_dataset.df)\n",
    "    print(\n",
    "        f\"epoch {epoch}, comp_score = {score_max:.3f}, mean_score = {score_mean:.3f},\"\n",
    "    )\n",
    "\n",
    "    # Update best scores and save the model if the mean score improves\n",
    "    if score_mean > best_score:\n",
    "        best_score = score_mean\n",
    "        best_score_max = score_max\n",
    "        torch.save(model.state_dict(), f\"best_model_{fold}.pth\")\n",
    "\n",
    "# Append the best scores for this fold to the respective lists\n",
    "score_means.append(best_score)\n",
    "score_maxs.append(best_score_max)\n",
    "\n",
    "# Calculate and display the mean scores across all folds\n",
    "print(\n",
    "    f\"comp_score = {np.mean(score_maxs)}, mean_score = {np.mean(score_means)},\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
