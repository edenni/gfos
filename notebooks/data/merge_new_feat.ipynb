{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_layout_root = Path(r\"H:\\data\\gfos\\predict-ai-model-runtime\\npz_all\\npz\\layout\")\n",
    "new_feat_root = Path(r\"H:\\data\\gfos\\predict-ai-model-runtime\\new_node_feat\")\n",
    "\n",
    "output_root = Path(r\"H:\\data\\gfos\\predict-ai-model-runtime\\npz_all\\npz\\layout_new\")\n",
    "\n",
    "output_root.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_new_feat(\n",
    "    original_layout_root: Path,\n",
    "    new_feat_root: Path,\n",
    "    output_root: Path,\n",
    "    source: str,\n",
    "    search: str,\n",
    "    split: str,\n",
    "    model_id: str,\n",
    "):\n",
    "    original_file = original_layout_root / source / search / split / f\"{model_id}.npz\"\n",
    "    new_feat_file = new_feat_root / source / split / f\"{model_id}.npy\"\n",
    "    save_dir = output_root / source / search / split\n",
    "    save_path = save_dir / f\"{model_id}.npz\"\n",
    "    \n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    data = dict(np.load(original_file))\n",
    "    new_feat = np.load(new_feat_file)\n",
    "    \n",
    "    assert len(data[\"node_feat\"]) == len(new_feat)\n",
    "    \n",
    "    data[\"node_feat\"] = new_feat\n",
    "    np.savez_compressed(save_path, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "for source_dir in original_layout_root.iterdir():\n",
    "    source = source_dir.name\n",
    "    for search_dir in source_dir.iterdir():\n",
    "        search = search_dir.name\n",
    "        for split_dir in search_dir.iterdir():\n",
    "            split = split_dir.name\n",
    "            params.extend(\n",
    "                [\n",
    "                    (\n",
    "                        original_layout_root,\n",
    "                        new_feat_root,\n",
    "                        output_root,\n",
    "                        source,\n",
    "                        search,\n",
    "                        split,\n",
    "                        file.stem,\n",
    "                    )\n",
    "                    for file in split_dir.glob(\"*.npz\")\n",
    "                ]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Parallel(n_jobs=8)(delayed(merge_new_feat)(*param) for param in tqdm(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp default test checked\n",
      "nlp default train checked\n",
      "nlp default valid checked\n",
      "nlp random test checked\n",
      "nlp random train checked\n",
      "nlp random valid checked\n",
      "xla default test checked\n",
      "xla default train checked\n",
      "xla default valid checked\n",
      "xla random test checked\n",
      "xla random train checked\n",
      "xla random valid checked\n"
     ]
    }
   ],
   "source": [
    "NUM_CHECK = 5\n",
    "\n",
    "for source_dir in original_layout_root.iterdir():\n",
    "    source = source_dir.name\n",
    "    for search_dir in source_dir.iterdir():\n",
    "        search = search_dir.name\n",
    "        for split_dir in search_dir.iterdir():\n",
    "            split = split_dir.name\n",
    "            checked = 0\n",
    "            for file in split_dir.glob(\"*.npz\"):\n",
    "                checked += 1\n",
    "                if checked > NUM_CHECK:\n",
    "                    break\n",
    "\n",
    "                new_file = output_root / source / search / split / file.name\n",
    "                \n",
    "                old_data = dict(np.load(file))\n",
    "                new_data = dict(np.load(new_file))\n",
    "                \n",
    "                for key in old_data.keys():\n",
    "                    if key != \"node_feat\":\n",
    "                        np.testing.assert_array_equal(old_data[key], new_data[key])\n",
    "                    else:\n",
    "                        np.testing.assert_array_equal(\n",
    "                            old_data[key], \n",
    "                            np.concatenate(\n",
    "                            [\n",
    "                                new_data[key][:, :134],\n",
    "                                new_data[key][:, -6:],\n",
    "                            ],\n",
    "                            axis=1,)\n",
    "                        )\n",
    "            print(f\"{source} {search} {split} checked\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
